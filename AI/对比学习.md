# 什么是对比学习
对比学习（Contrastive Learning）是一种**自监督学习方法**，通过学习数据之间的对比信息来构建有效的特征表示。它的核心思想是将相似的数据样本近（即使它们的表示更加相似），而将不相似的数据样本推远（即使它们的表示更加不同）。对比学习在无监督学习和表征学习（Representation Learning）中非常有效，特别是在图像、文本等领域。
![](https://pic.imgdb.cn/item/66835cb4d9c307b7e9dc2af7.png)

# 核心思想
对比学习的主要目标是构建一个编码器，使得**相似的输入样本在嵌入空间中靠近，不相似的样本在嵌入空间中远离**。它通常包括以下几个步骤：
- 数据增强：对每个输入样本进行多种数据增强操作，生成不同的视图（视角、增强后的样本）。
- 特征表示：通过神经网络（如卷积神经网络）对增强后的样本进行编码，得到其嵌入表示。
- 对比损失：定义一个对比损失函数来度量嵌入表示之间的相似度，通过优化这个损失函数来训练模型。
![](https://pic.imgdb.cn/item/66835db6d9c307b7e9dd7b7f.png)

> 那哪些图片算负样本呢？剩下的所有不是这张图的类别，所有叫做Instance discrimination（个体判别）。
![](https://pic.imgdb.cn/item/668361ecd9c307b7e9e3d047.png)

# 常见对比学习方法 
## SimCLR（Simple Framework for Contrastive Learning of Visual Representations）
SimCLR 是一种简单而有效的对比学习方法，通过最大化不同视图（经过数据增强后的样本）的表示之间的相似性来学习特征。其基本步骤如下：

- 数据增强：对每个样本进行两次随机数据增强，生成两个视图。
- 特征提取：使用一个共享的神经网络对增强后的样本进行编码，得到其嵌入表示。
- 对比损失：使用对比损失（如NT-Xent损失）来度量和最大化同一样本不同视图的嵌入表示之间的相似性，同时最小化不同样本之间的相似性。
  
SimCLR 的对比损失函数定义如下：

![](https://pic.imgdb.cn/item/66835f5cd9c307b7e9e058be.png)

其中,zi和𝑧𝑗是同一图像的两个视图的嵌入表示,sim(⋅,⋅) 是相似度函数（通常是余弦相似度），
𝜏是温度参数。

## MoCo
MoCo（Momentum Contrast）是一种对比学习方法，它使用动量编码器（Momentum Encoder）来**生成更稳定的负样本嵌入表示**，从而提高训练效率和效果。MoCo的主要目标是通过动量更新的负样本队列和对比损失函数来学习有效的特征表示。
### MoCo的主要思想
1. 动量编码器（Momentum Encoder）： MoCo引入了一个动量编码器，它是一个逐步更新的模型副本。动量编码器的参数是通过主编码器（Query Encoder）的参数以较低的学习率逐步更新的。这种动量更新机制可以产生更稳定的负样本表示。
2. 负样本队列（Negative Sample Queue）： MoCo维护了一个动态更新的负样本队列，保存了一定数量的最近计算的负样本嵌入表示。通过这种方式，可以在每个训练步骤中使用大量的负样本，从而增强对比学习的效果。
3. 对比损失（Contrastive Loss）： MoCo使用对比损失来最大化正样本对（同一图像的不同视图）的相似性，同时最小化正样本和负样本之间的相似性。常用的对比损失是InfoNCE（Noise Contrastive Estimation）损失。
### MoCo的训练过程
1. 数据增强： 对每个输入样本进行两次数据增强，生成两个视图：查询视图（query view）和键视图（key view）。
2. 特征表示： 使用主编码器对查询视图进行编码，使用动量编码器对键视图进行编码，得到它们的嵌入表示。
3. 动量更新： 逐步更新动量编码器的参数，使其接近主编码器的参数。**动量其实就是一种加权平均**,让动量编码器较为缓慢的更新。
   ![](https://pic.imgdb.cn/item/668362b0d9c307b7e9e4d412.png)

4. 更新负样本队列： 将当前批次的键视图嵌入表示添加到负样本队列中，并从队列中移除最旧的样本。
5. 计算对比损失： 通过对比损失函数来度量查询视图和键视图的相似性，并进行优化。对比损失函数为InfoNCE损失函数：

![](https://pic.imgdb.cn/item/668360ead9c307b7e9e27f68.png)

### 为什么视觉领域的无监督学习就是在build dynamic dictionary？
![无监督学习到对比学习](https://pic.imgdb.cn/item/668379d6d9c307b7e909c12f.png)
无监督学习就是将正样本编码，然后去一个装有所有其他样本的编码字典里去找 和自己一个源样本的样本，而训练的query encoder就是为了这个目的，通过contrastive loss来使相似的样本靠近，不相似的样本远离。

### 为什么字典要大并且保证一致性？
- 字典越大，query就能够和更多的key进行对比，就更有可能学到区分物体的最本质的特征。
- 一致性指keys要用相同或者相似的编码器得到。这样得到的key和query比较才能学习到代表样本的语义信息，而不是由于模型不同引入的其他有代表性的信息，避免模型学到一些trivial solution，得到一些捷径解。

Moco就是为了构造又大又一致的字典。

![MOCO架构图](https://pic.imgdb.cn/item/66837c53d9c307b7e90f4acc.png)

### 为什么需要队列？
- 字典中需要保存语义丰富的负样本，这样能够学习到用于判别的关键特征。
- 使用队列可以利用之前mini-batch计算出的key。
- 队列大小和mini batchsize解绑，query不一定只和mini batch中key对比。同时，队列大小可配置为超参数，很灵活。
- 队列有先进先出的特性，从一致性角度来说，最早计算的mini-batch是最过时的，也就是和最新计算出来的key是最不一致的。

### 为什么要缓慢更新或者说动量更新？
因为要使得中间学习的字典特征尽量保持一致。假设mini-batch size=1，那么队列中的每个key都是由不同参数的encoder产生的，这样队列中一致性很差。
# 参考
- [【硬核科普】6分钟了解对比学习(Contrast Learning)](https://www.bilibili.com/video/BV1zr4y1b7F9)
- [MoCo 论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1C3411s7t9)（moco动量编码器的必要性能从这里知道）