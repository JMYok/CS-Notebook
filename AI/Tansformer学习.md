# 什么是Transformer
Transformer是一种基于**注意力机制**的神经网络架构，最初由Vaswani等人于2017年提出，主要用于序列到序列（sequence-to-sequence）的自然语言处理（NLP）任务，如机器翻译、文本生成等。Transformer架构的**核心思想是完全基于注意力机制**，而不依赖于传统的循环神经网络（RNN）或卷积神经网络（CNN）。

Transformer的关键创新之一是自注意力机制（self-attention），它允许模型在计算隐藏表示时动态地考虑输入序列中不同位置之间的关系。这种机制使得Transformer能够：
1. **并行处理整个序列，加速训练过程**。RNN只能按照时序顺序处理，无法利用GPU的并行能力。
2. **能够捕捉长距离依赖关系**。RNN会丢失长距离特征。

# CNN & Transformer
- CNN对于长序列的处理需要多次卷积，例如图像中两点距离较远，需要多次卷积才能实现融合。
- CNN的优势是能够实现多通道，各个通道代表一种模式，借助这种思想，Transformer提出多头注意力机制，模拟CNN的多通道的效果。

# Transformer架构
1. 编码器（Encoder）： 编码器接受输入序列，并将其转换为一系列的隐藏表示。每个输入单词或标记都通过**自注意力机制**（self-attention）与其他单词或标记进行交互，以捕捉输入序列中的相关信息。编码器通常由多个堆叠的相同结构的层组成，每个层包含多头注意力模块（multi-head attention）和前馈神经网络模块（feed-forward neural network）。

2. 解码器（Decoder）： 解码器接受编码器产生的隐藏表示，并生成输出序列。与编码器类似，解码器也由多个堆叠的层组成，每个层包含自注意力模块、编码-解码注意力模块（encoder-decoder attention）和前馈神经网络模块。解码器的自注意力模块用于捕捉输出序列中的上下文信息，而编码-解码注意力模块用于在解码阶段对编码器的隐藏表示进行引导。

3. 位置编码（Positional Encoding）： 由于Transformer架构没有像循环神经网络那样显式地处理序列顺序，因此需要引入位置编码来提供输入序列中单词或标记的位置信息。
![](https://pic.imgdb.cn/item/6684e1a2d9c307b7e9ab2eaf.png)

## 编码器


## 解码器