# 什么是分布式
分布式其实就是**将相同或相关的程序运行在多台计算机上，从而实现特定目标的一种计算方式**。

从这个定义来看，数据并行、任务并行其实都可以算作是分布式的一种形态。从这些计算方式的演变中不难看出，产生分布式的最主要驱动力量，是我们对于性能、可用性及可扩展性的不懈追求。

# RPC(远程调用)
分布式的本质就是**多进程协作**，共同完成任务。要协作，自然免不了通信。那么，多个进程之间是如何通信的呢？
![](https://pic.imgdb.cn/item/66a06608d9c307b7e9d0c7d9.png)
本地调用通常指的是，**进程内函数之间的相互调用**；而远程调用，是**进程间函数的相互调用**，是一种进程间通信模式。

在分布式领域中，**一个系统由很多服务组成，不同的服务由各自的进程单独负责**。因此，远程调用在分布式通信中尤为重要。

根据进程是否部署在同一台机器上，远程调用可以分为如下两类：

- 本地过程调用（Local Procedure Call，LPC），是指同一台机器上运行的不同进程之间的互相通信，即在多进程操作系统中，运行的不同进程之间可以通过LPC进行函数调用。
- **远程过程调用（Remote Procedure Call，RPC）**，是指不同机器上运行的进程之间的相互通信，**某一机器上运行的进程在不知道底层通信细节的情况下，就像访问本地服务一样，去调用远程机器上的服务**。


![](https://pic.imgdb.cn/item/66a06837d9c307b7e9d2c0b1.png)
RPC的目的，其实就是要将第2到第8步的几个过程封装起来，让用户看不到这些细节。从用户的角度看，订单系统的进程只是做了一次普通的本地调用，然后就得到了结果。

也就是说，订单系统进程并不需要知道底层是如何传输的，**在用户眼里，远程过程调用和调用一次本地服务没什么不同。这，就是RPC的核心。**

另外，RPC是一种抽象的 网络服务协议框架（即定义网络中各个服务如何通信的规则），是有多种实现的。

# CAP理论
- **C代表Consistency，一致性，是指所有节点在同一时刻的数据是相同的**，即更新操作执行结束并响应用户完成后，所有节点存储的数据会保持相同。
    > 电商系统中，A、B、C中存储的该电吹风的数量应该是20+10+30=60。假设，现在有一个北京用户买走一个电吹风，服务器A会更新数据为60-1=59，与此同时要求B和C也更新为59，以保证在同一时刻，无论访问A、B、C中的哪个服务器，得到的数据均是59。
- **A代表Availability，可用性，是指系统提供的服务一直处于可用状态，对于用户的请求可即时响应。**
  > 在电商系统中，用户在任一时刻向A、B、C中的任一服务器发出请求时，均可得到即时响应，比如查询商品信息等。
- **P代表Partition Tolerance，分区容错性，是指在分布式系统遇到网络分区的情况下，仍然可以响应用户的请求。** 网络分区是指因为网络故障导致网络不连通，不同节点分布在不同的子网络中，各个子网络内网络正常。在实际场景中，网络环境不可能百分之百不出故障，比如网络拥塞、网卡故障等，会导致网络故障或不通，**从而导致节点之间无法通信，或者集群中节点被划分为多个分区，分区中的节点之间可通信，分区间不可通信。**
这种由网络故障导致的集群分区情况，通常被称为“网络分区”。
    > 在电商系统中，假设C与A和B的网络都不通了，A和B是相通的。也就是说，形成了两个分区{A, B}和{C}，在这种情况下，系统仍能响应用户请求。

![](https://pic.imgdb.cn/item/66a06fd3d9c307b7e9da22b0.png)
CAP理论指的就是，**在分布式系统中C、A、P这三个特征不能同时满足，只能满足其中两个**。

在分布式系统中，网络分区不可避免，因此分区容错性P必须满足。
- 保证一致性C，牺牲可用性A
  ![](https://pic.imgdb.cn/item/66a0784ad9c307b7e9e1be15.png)
  > 例如zookeeper处理网络中的写请求：当出现网络分区时，如果其中一个分区的节点数大于集群总节点数的一半，那么这个分区可以再选出一个Leader，仍然对用户提供服务，但在选出Leader之前，不能正常为用户提供服务；如果形成的分区中，没有一个分区的节点数大于集群总节点数的一半，那么系统不能正常为用户提供服务，必须待网络恢复后，才能正常提供服务。
- 保证可用性A，牺牲一致性C
  ![](https://pic.imgdb.cn/item/66a07872d9c307b7e9e1e211.png)
  > 以电商购物系统为例，如下图所示，某电吹风在北京仓库有20个，在杭州仓库有10个，在上海仓库有30个。初始时，北京、杭州、上海分别建立的服务器{A, B, C}存储该电吹风的数量均为60个。
  >
  > 假如，上海的网络出现了问题，与北京和杭州网络均不通，此时北京的用户通过北京服务器A下单购买了一个电吹风，电吹风数量减少到59，并且同步给了杭州服务器B。也就是说，现在用户的查询请求如果是提交到服务器A和B，那么查询到的数量为59。但通过上海服务器C进行查询的结果，却是60。
  > 
  > 当然，待网络恢复后，服务器A和B的数据会同步到C，C更新数据为59，最终三台服务器数据保持一致，用户刷新一下查询界面或重新提交一下查询，就可以得到最新的数据。而对用户来说，他们并不会感知到前后数据的差异，到底是因为其他用户购买导致的，还是因为网络故障导致数据不同步而产生的。
  ![](https://pic.imgdb.cn/item/66a07a55d9c307b7e9e38877.png)
  为什么上海服务器不能等网络恢复后，再响应用户请求呢？可以想象一下，如果用户提交一个查询请求，需要等上几分钟、几小时才能得到反馈，那么用户早已离去了。

# BASE理论
> eBay 的架构师 Dan Pritchett 源于对大规模分布式系统的实践总结，在 ACM 上发表文章提出 BASE 理论，BASE 理论是对 CAP 理论的延伸，核心思想是即使无法做到强一致性（Strong Consistency，CAP 的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）。

- **Basically Available（基本可用）** 分布式系统在出现不可预知故障的时候，允许损失部分可用性。
  > - 响应时间上的损失: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。
  > - 系统功能上的损失：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。

- **Soft state（软状态）软状态也称为弱状态**，和硬状态相对，是指**允许系统中的数据存在中间状态（CAP 理论中的数据不一致）**，并认为该中间状态的存在不会影响系统的整体可用性，即**允许系统在不同节点的数据副本之间进行数据同步的过程存在延时**。
- **Eventually consistent（最终一致性）** 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。
  > 分布式一致性的 3 种级别：
  > 1. 强一致性：系统写入了什么，读出来的就是什么。
  > 2. 弱一致性：不一定可以读取到最新写入的值，也不保证多少时间之后读取到的数据是最新的，只是会尽量保证某个时刻达到数据一致的状态。
  > 3. 最终一致性：弱一致性的升级版，**系统会保证在一定时间内达到数据一致的状态**。
  > 
  > 业界比较推崇是最终一致性级别，但是某些对数据一致要求十分严格的场景比如银行转账还是要保证强一致性。

## BASE和CAP的关系
![](https://pic.imgdb.cn/item/66a07c41d9c307b7e9e63d0b.png)
AP 方案只是在**系统发生分区的时候放弃一致性，而不是永远放弃一致性**。在分区故障恢复后，系统应该达到最终一致性。这一点其实就是 BASE 理论延伸的地方。

所以BASE理论表示，**数据不一致时的中间状态不影响整体的系统运行，系统可允许损失部分可用性而保证整体的可用性，同时，数据只用最终达到一致即可**。

# Paxos算法
## Basic Paxos
![](https://pic.imgdb.cn/item/66a081a6d9c307b7e9eaf272.png)
- 提议者（Proposer）：提议一个值，用于投票表决。
- 接受者（Acceptor）：对每个提议的值进行投票，并存储接受的值。 一般来说，集群中的所有节点都在扮演接受者的角色，参与共识协商，并接受和存储数据。
- 学习者（Learner）：被告知投票的结果，接受达成共识的值，存储保存，不参与投票的过程。一般来说，学习者是数据备份节点，比如“Master-Slave”模型中的 Slave，被动地接受数据，容灾备份。

一个节点可以身兼多种角色。

这三种角色，在本质上代表的是三种功能：
1. 提议者代表的是接入和协调功能，收到客户端请求后，发起二阶段提交，进行共识协商；（收到客户端请求的服务器节点）
2. 接受者代表投票协商和存储数据，对提议的值进行投票，并接受达成共识的值，存储保存；（理解为服务器节点）
3. 学习者代表存储数据，不参与共识协商，只接受达成共识的值，存储保存。（理解为数据库节点）

### 如何达成共识？
#### 准备（Prepare）阶段
![](https://pic.imgdb.cn/item/66a20a81d9c307b7e93e2d26.png)
> 准备请求中是不需要指定提议的值的，只需要携带提案编号就可以了

![](https://pic.imgdb.cn/item/66a20b44d9c307b7e93ee0e4.png)

由于之前没有通过任何提案，所以节点 A、B 将返回一个 “尚无提案”的响应。也就是说节点 A 和 B 在告诉提议者，我之前没有通过任何提案呢，并承诺以后不再响应提案编号小于等于 1 的准备请求，不会通过编号小于 1 的提案

> 可以认为，数越大，优先级越高。 所以，5号可以打断1号，而1号不能打断5号。 这里，也就是“越新的，优先级越高”，肯定是保留最新的值。

![](https://pic.imgdb.cn/item/66a20c6ad9c307b7e93fef2b.png)

#### 接受（Accept）阶段
![](https://pic.imgdb.cn/item/66a20cebd9c307b7e9406416.png)

当客户端 1 收到大多数的接受者（节点 A、B）的准备响应后，根据响应中提案编号最大的提案的值，设置接受请求中的值。因为该值在来自节点 A、B 的准备响应中都为空，所以就把自己的提议值 3 作为提案的值，发送接受请求[1, 3]。客户端2同理。

![](https://pic.imgdb.cn/item/66a20d6cd9c307b7e940d6a0.png)
- 当节点 A、B、C 收到接受请求[1, 3]的时候，由于提案的提案编号 1 小于三个节点承诺能通过的提案的最小提案编号 5，所以提案[1, 3]将被拒绝。
- 当节点 A、B、C 收到接受请求[5, 7]的时候，由于提案的提案编号 5 不小于三个节点承诺能通过的提案的最小提案编号 5，所以就通过提案[5, 7]，也就是接受了值 7，三个节点就 X 值为 7 达成了共识。
- 如果集群中有学习者，当接受者通过了一个提案时，就通知给所有的学习者。当学习者发现大多数的接受者都通过了某个提案，那么它也通过该提案，接受该提案的值。
- 最终各节点就 X 的值达成了共识。Basic Paxos 的容错能力，源自“大多数”的约定，你可以这么理解：当少于一半的节点出现故障的时候，共识协商仍然在正常工作。

## Multi-Paxos
Basic Paxos 只能就单个值（Value）达成共识，一旦遇到为一系列的值实现共识的时候，它就不管用了。

兰伯特提到可以通过多次执行 Basic Paxos 实例（比如每接收到一个值时，就执行一次 Basic Paxos 算法）实现一系列值的共识。

兰伯特提到的 **Multi-Paxos 是一种思想，不是算法**。而 Multi-Paxos 算法是一个统称，它是指基于 Multi-Paxos 思想，通过多个 Basic Paxos 实例实现一系列值的共识的算法（比如 Chubby 的 Multi-Paxos 实现、Raft 算法等）
### 问题
如果直接通过多次执行 Basic Paxos 实例来达到共识有两个问题：
1. 如果多个提议者同时提交提案，可能出现因为提案冲突，在准备阶段没有提议者收到大多数准备响应，协商失败，这样就需要重新协商。
   > 这里指的是提案编号相同、“选票”被瓜分的情况。另外，编号最大的提议者，虽然能收到大多数准备响应，但也可能因为各提议者不断递增提案编号和重试，出现始终没有提案被选定的情况。
2. 因为准备阶段和接受阶段会进行两轮RPC通讯，往返消息多，耗性能，延迟大，这是需要优化的。

### 如何解决这两个问题呢？ 
1. 引入领导者，让领导者作为唯一提议者，这里涉及到如何选举领导者，不同的算法可以有不同的实现方法 Chubby 是通过执行 Basic Paxos算法投票选举Raft通过一个随机倒计时功能，最快得到大多数投票的为领导者。
2. 优化Basic Paxos执行采用当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段。

### Raft算法

### 工程案例

# 参考
- 分布式协议与算法实战-极客时间
- 分布式技术原理与算法解析-极客时间
- [分布式理论 - BASE](https://pdai.tech/md/dev-spec/spec/dev-th-base.html)
- [CAP & BASE理论详解](https://javaguide.cn/distributed-system/protocol/cap-and-base-theorem.html)
- 
